{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f711e8",
   "metadata": {
    "executionInfo": {
     "elapsed": 2378,
     "status": "ok",
     "timestamp": 1628558321666,
     "user": {
      "displayName": "이영석",
      "photoUrl": "",
      "userId": "05938684673482326731"
     },
     "user_tz": -540
    },
    "id": "d018142b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "from tqdm import tqdm \n",
    "\n",
    "from konlpy.tag import Okt\n",
    "from konlpy.tag import Hannanum\n",
    "from konlpy.tag import Kkma\n",
    "from konlpy.tag import Komoran\n",
    "from konlpy.tag import Mecab\n",
    "\n",
    "okt = Okt()\n",
    "hannanum=Hannanum()\n",
    "kkma = Kkma()\n",
    "komoran = Komoran()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d018142b",
   "metadata": {
    "executionInfo": {
     "elapsed": 2378,
     "status": "ok",
     "timestamp": 1628558321666,
     "user": {
      "displayName": "이영석",
      "photoUrl": "",
      "userId": "05938684673482326731"
     },
     "user_tz": -540
    },
    "id": "d018142b"
   },
   "outputs": [],
   "source": [
    "#댓글 불러온 뒤 앞에 best글자 없애기, 한글과 빈칸 제외하고 모두 삭제하기\n",
    "blog = pd.read_csv('C:/Users/twentystones/Desktop/사회연결망분석텀프로젝트/blog_crawling.csv', header=None)\n",
    "blog.rename(columns= {0:'title', 1:'URL', 2:'day', 3:'body'}, inplace=True)\n",
    "\n",
    "'''\n",
    "for k in tqdm(range(0,len(comment['best_comment']))):\n",
    "    comment['best_comment'][k] = re.sub(\"BEST\", \"\", comment['best_comment'][k])\n",
    "\n",
    "comment['best_comment'] = comment['best_comment'].str.replace(\"[^ㄱ-ㅎㅏ-ㅣ가-힣 ]\",\"\") # 정규 표현식 수행\n",
    "comment.to_csv('best_removedAndRegulared')   #중간에 저장하기\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "#불용어\n",
    "stop_words = ['저','거','의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다','..',\n",
    "              '진짜', '사람', '웹툰', '그냥', '보고', '정도', '정말', '머리', '이번', '이제', '만화', '그림', '때문']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c97ec22",
   "metadata": {
    "id": "9c97ec22"
   },
   "source": [
    "#분석하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "qM4gJcEapTUq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "output_embedded_package_id": "1uJMsk5Mj6NVH82RYK9fNOVGoVQdVHAKE"
    },
    "executionInfo": {
     "elapsed": 3397377,
     "status": "ok",
     "timestamp": 1628579131896,
     "user": {
      "displayName": "이영석",
      "photoUrl": "",
      "userId": "05938684673482326731"
     },
     "user_tz": -540
    },
    "id": "qM4gJcEapTUq",
    "outputId": "179214b0-b7a8-42d2-8b18-9a16238b8808"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('도쿄', 232), ('등심', 124), ('시계', 79), ('느낌', 68), ('생각', 66), ('모델', 66), ('브랜드', 66), ('일본', 64), ('한우', 58), ('고기', 57), ('요리', 51), ('오늘', 45), ('하나', 45), ('긴자', 41), ('매장', 40), ('분위기', 39), ('코스', 37), ('소스', 35), ('시간', 35), ('한국', 34), ('오모테산도', 30), ('사진', 30), ('여기', 28), ('가격', 28), ('최근', 26), ('한번', 25), ('바로', 25), ('광교', 25), ('새우', 24), ('식사', 24), ('가장', 24), ('가지', 24), ('와인', 23), ('전복', 23), ('가게', 23), ('다음', 22), ('워치', 22), ('여행', 22), ('맛집', 22), ('구이', 22), ('크림', 21), ('치즈', 21), ('우리', 21), ('여의도', 21), ('구경', 20), ('제공', 20), ('가을', 20), ('거리', 20), ('드라이브', 20), ('장소', 20), ('제품', 20), ('이지', 20), ('요즘', 19), ('먼저', 19), ('약간', 19), ('지금', 19), ('카페', 19), ('미역국', 19), ('정원', 19), ('고로케', 18), ('조금', 18), ('세이', 18), ('방문', 17), ('인기', 17), ('그랜드', 17), ('공원', 17), ('도곡', 17), ('관광', 17), ('쇼핑', 16), ('친구', 16), ('추가', 16), ('이야기', 16), ('전시', 16), ('데이트', 16), ('소고기', 16), ('스키야키', 16), ('공간', 16), ('시작', 16), ('처음', 15), ('오픈', 15), ('이건', 15), ('메뉴', 15), ('겨울', 15), ('디자인', 15), ('자주', 15), ('다른', 15), ('먹기', 15), ('아이스크림', 15), ('선릉', 15), ('예약', 14), ('계속', 14), ('위치', 14), ('마음', 14), ('구매', 14), ('상품', 14), ('근처', 14), ('한정', 14), ('어쨌든', 14), ('경우', 14), ('저녁', 14)]\n"
     ]
    }
   ],
   "source": [
    "#웹툰 전체 키워드 뽑기\n",
    "keyword=[]\n",
    "\n",
    "#웹툰 전체 댓글을 한 문장에 합치기\n",
    "for sentence in tqdm(blog['body']):  \n",
    "    try :\n",
    "        okt_nouns = okt.nouns(sentence)\n",
    "    #불용어와 1글자 빼기\n",
    "        for i,v in enumerate(okt_nouns):\n",
    "            if len(v)<2 or v in stop_words:\n",
    "                pass\n",
    "            else :\n",
    "                keyword.append(v)\n",
    "    except :\n",
    "        print('error')\n",
    "\n",
    "count1 = Counter(keyword)\n",
    "\n",
    "noun_list1 = count1.most_common(100)  #갯수조절\n",
    "print(noun_list1)\n",
    "\n",
    "#저장\n",
    "#list_df = pd.DataFrame({'keyword' :noun_list1})\n",
    "#list_df.to_csv('keyword_all_webtoon_att1.csv', encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Czit8F7BpifP",
   "metadata": {
    "id": "Czit8F7BpifP"
   },
   "outputs": [],
   "source": [
    "#웹툰 작품별 키워드 뽑기\n",
    "keyword=[]\n",
    "b=0\n",
    "for i in code_list['code']:\n",
    "    b+=1\n",
    "    each_comment = comment.loc[comment['code_b'] == i]\n",
    "    each_comment = each_comment['best_comment']\n",
    "    #행별 문장들을 한문장으로 합치기\n",
    "    line_sum =\". \"\n",
    "    for t in each_comment:\n",
    "        line_sum = line_sum+str(t)+\". \"\n",
    "    print(line_sum)\n",
    "\n",
    "    okt_nouns = okt.nouns(line_sum)\n",
    "    for i,v in tqdm(enumerate(okt_nouns)):\n",
    "        if len(v)<2 or v in stop_words:\n",
    "            okt_nouns.pop(i)\n",
    "          \n",
    "    count1 = Counter(okt_nouns)\n",
    "\n",
    "    noun_list1 = count1.most_common(100)\n",
    "    for a in noun_list1:\n",
    "        print(a)\n",
    "    list_df = pd.DataFrame({'keyword' :noun_list1})\n",
    "    list_df.to_csv('keyword'+str(b)+'.csv', encoding = 'utf-8-sig')  #중간에 오류날까봐 한개씩 저장\n",
    "\n",
    "#저장했던 것들 모두 합치기\n",
    "df = pd.DataFrame({})\n",
    "for k in range(1,332):\n",
    "    sr = pd.read_csv('/content/keyword'+str(k)+'.csv')['keyword']\n",
    "    df = pd.concat([df, sr], axis=1)\n",
    "\n",
    "#칼럼네임\n",
    "for j in range(0,331):\n",
    "    df.columns.values[j] = str(code_list['code'][j]) +'  '+ str(code_list['title'][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PHHXu03K0ki9",
   "metadata": {
    "id": "PHHXu03K0ki9"
   },
   "outputs": [],
   "source": [
    "#웹툰 회차별 키워드 뽑기  \n",
    "keyword=[]\n",
    "list_df = pd.DataFrame({})\n",
    "\n",
    "for n in range(0,331):\n",
    "    #for m in range(n*15:(n+1)*15):\n",
    "    for sentence in comment['best_comment'][n*15:(n+1)*15]:\n",
    "        try :\n",
    "            okt_nouns = okt.nouns(sentence)\n",
    "        #불용어와 1글자 빼기\n",
    "            for i,v in enumerate(okt_nouns):\n",
    "                if len(v)<2 or v in stop_words:\n",
    "                    pass\n",
    "                else :\n",
    "                    keyword.append(v)\n",
    "        except :\n",
    "            print('error')        \n",
    "    count = Counter(keyword)\n",
    "    noun_list+n = count.most_common(100)  #갯수조절\n",
    "    print(noun_list+n)\n",
    "\n",
    "\n",
    "list_df.to_csv('keyword'+str(b)+'.csv', encoding = 'utf-8-sig')\n",
    "        \n",
    "        \n",
    "for code in code_list['code']:\n",
    "    for ep in \n",
    "    \n",
    "    each_comment = comment.loc[comment['code_b'] == code]\n",
    "  \n",
    "    for ep in each_comment  \n",
    "\n",
    "    each_comment = each_comment['best_comment']\n",
    "  #행별 문장들을 한문장으로 합치기\n",
    "    line_sum =\". \"\n",
    "    for t in each_comment:\n",
    "        line_sum = line_sum+str(t)+\". \"\n",
    "    print(line_sum)\n",
    "\n",
    "    okt_nouns = okt.nouns(line_sum)\n",
    "    for i,v in tqdm(enumerate(okt_nouns)):\n",
    "        if len(v)<2 or v in stop_words:\n",
    "            okt_nouns.pop(i)\n",
    "          \n",
    "    count1 = Counter(okt_nouns)\n",
    "\n",
    "    noun_list1 = count1.most_common(100)\n",
    "    for a in noun_list1:\n",
    "        print(a)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "주요키워드 찾기(미완성).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

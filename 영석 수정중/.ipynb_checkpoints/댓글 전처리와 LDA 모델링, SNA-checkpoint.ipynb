{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FBRFqY0ScHvt"
   },
   "source": [
    "# 한글 댓글 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ofhYXGNMcHvx"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/gdrive/Shareddrives/데청캠/웹툰 세계화/best_removedAndRegulared한글베댓 베스트지우고 한글과공백만 남김.csv')\n",
    "df = df.iloc[:,2:]\n",
    "df = df.fillna('')\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TyVD0ZWYcHvz"
   },
   "outputs": [],
   "source": [
    "# 명사 추출\n",
    "comment_mecab_nouns = []\n",
    "for i in range(len(df)):\n",
    "  comment_mecab_nouns.append(mecab.nouns(df.best_comment[i]))\n",
    "  print(i)\n",
    "df['comment_mecab_nouns'] = comment_mecab_nouns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6QKypwyocHv0"
   },
   "outputs": [],
   "source": [
    "#한글자짜리 지우기\n",
    "comment_mecab_nouns_len = []\n",
    "for i in range(len(df)):\n",
    "  comment_mecab_nouns_len.append([])\n",
    "\n",
    "for i in range(len(comment_mecab_nouns)):\n",
    "  for word in comment_mecab_nouns[i]:\n",
    "    if len(word)!=1:\n",
    "      comment_mecab_nouns_len[i].append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PAKfrSSYcHv0"
   },
   "outputs": [],
   "source": [
    "# 지칭 불용어 제거\n",
    "stop_words_noun = ['사람','다음','정도', '하나', '이거', '여기', '이번', '인간', '어디','겁니다','이게','저거','이건','걸로','그거','그걸','그게','저기','그것','이것','저것','무엇','어디']\n",
    "comment_mecab_nouns_len_stp = []\n",
    "for i in range(len(df)):\n",
    "  comment_mecab_nouns_len_stp.append([])\n",
    "\n",
    "for i in range(len(df)):\n",
    "  for word in comment_mecab_nouns_len[i]:\n",
    "    if word not in stop_words_noun:\n",
    "      comment_mecab_nouns_len_stp[i].append(word)\n",
    "df['comment_mecab_nouns_len_stp'] = comment_mecab_nouns_len_stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h1czWti1cHv1"
   },
   "outputs": [],
   "source": [
    "# 특수문자 제거\n",
    "df.comment_mecab_nouns_len_stp = df.comment_mecab_nouns_len_stp.str.replace(pat=r'[^\\w]', repl=r' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmeDyXCwcHv2"
   },
   "outputs": [],
   "source": [
    "# 숫자 제거\n",
    "df.comment_mecab_nouns_len_stp = df.comment_mecab_nouns_len_stp.str.replace(r'\\d+','')\n",
    "comment_mecab_nouns_len_stp = df.comment_mecab_nouns_len_stp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LczitnlDcHv3"
   },
   "outputs": [],
   "source": [
    "# bigram 모델 생성\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "data_words_bigrams = make_bigrams(comment_mecab_nouns_len_stp)\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "texts = data_words_bigrams\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nSdYoSX_cHv5"
   },
   "outputs": [],
   "source": [
    "#LDA\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "ldamallet = LdaMallet(mallet_path, corpus=corpus, num_topics=30, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wI7zi9btcHv7"
   },
   "outputs": [],
   "source": [
    "# coherence 평가\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "  \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "#k = 5,10,15,20 한 결과 k = 10 선택\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_words_bigrams, start=5, limit=25, step=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucpNrgCFcHv9"
   },
   "outputs": [],
   "source": [
    "# document 별 topic 확률 확인하는 dataframe 만들기\n",
    "idx,topic0,topic1,topic2,topic3,topic4,topic5,topic6,topic7,topic8,topic9=[],[],[],[],[],[],[],[],[],[],[]\n",
    "for i in range(len(corpus)):\n",
    "  document_topics = gensimmodel10.get_document_topics(corpus[i])\n",
    "  idx.append(i)\n",
    "  topic0.append(document_topics[0][1])\n",
    "  topic1.append(document_topics[1][1])\n",
    "  topic2.append(document_topics[2][1])\n",
    "  topic3.append(document_topics[3][1])\n",
    "  topic4.append(document_topics[4][1])\n",
    "  topic5.append(document_topics[5][1])\n",
    "  topic6.append(document_topics[6][1])\n",
    "  topic7.append(document_topics[7][1])\n",
    "  topic8.append(document_topics[8][1])\n",
    "  topic9.append(document_topics[9][1])\n",
    "df_topic_probs_df['idx'] = idx\n",
    "df_topic_probs_df['topic0'] = topic0\n",
    "df_topic_probs_df['topic1'] = topic1\n",
    "df_topic_probs_df['topic2'] = topic2\n",
    "df_topic_probs_df['topic3'] = topic3\n",
    "df_topic_probs_df['topic4'] = topic4\n",
    "df_topic_probs_df['topic5'] = topic5\n",
    "df_topic_probs_df['topic6'] = topic6\n",
    "df_topic_probs_df['topic7'] = topic7\n",
    "df_topic_probs_df['topic8'] = topic8\n",
    "df_topic_probs_df['topic9'] = topic9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfWoZIcxcHv_"
   },
   "outputs": [],
   "source": [
    "# document별로 topic 확률을 토대로 속성 확률 구하기\n",
    "df_topic_probs_df['att1'] = (df_topic_probs_df['topic0']+ df_topic_probs_df['topic1']+ df_topic_probs_df['topic3']+ df_topic_probs_df['topic6'] + df_topic_probs_df['topic9'])/5\n",
    "df_topic_probs_df['att2'] = (df_topic_probs_df['topic2']+df_topic_probs_df['topic5']+df_topic_probs_df['topic7'])/3\n",
    "df_topic_probs_df['att3'] = (df_topic_probs_df['topic4']+df_topic_probs_df['topic8'])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pUPtSuelcHwA"
   },
   "outputs": [],
   "source": [
    "df_topic_probs_df = df_topic_probs_df[['att1','att2','att3']]\n",
    "df_topic_probs_df_argmax = []\n",
    "for i in range(len(df_topic_probs_df)):\n",
    "  df_topic_probs_df_argmax.append(np.argmax(df_topic_probs_df.loc[i]))\n",
    "df_topic_probs_df['argmax'] = df_topic_probs_df['argmax'] +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d3w0DkZUcHwC"
   },
   "outputs": [],
   "source": [
    "df['attribute_num'] = df_topic_probs_df.argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1dGDqqWcHwE"
   },
   "source": [
    "# 영어 댓글 LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8x_w4fScHwF"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('/content/gdrive/Shareddrives/데청캠/웹툰 세계화/데이터 모음집/ew_2_comment.csv')\n",
    "df = df[['title_a','ep_title_a','num_list','best_comment']]\n",
    "for i in range(len(df)):\n",
    "  if df['best_comment'][i][:3]=='TOP':\n",
    "    df['best_comment'][i] = df['best_comment'][i][3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-sdugW4cHwF"
   },
   "outputs": [],
   "source": [
    "# 명사 tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "result_nouns=[]\n",
    "for i in range(len(df)):\n",
    "  result_nouns.append([])\n",
    "\n",
    "for i in range(len(df)):\n",
    "  txt =  df.best_comment[i]\n",
    "  tokenized = nltk.word_tokenize(txt)\n",
    "  for word,pos in nltk.pos_tag(tokenized):\n",
    "    if pos[:2] == 'NN':\n",
    "      result_nouns[i].append(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KejkpleScHwG"
   },
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords  \n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "result_nouns_stopwords=[]\n",
    "for i in range(len(result_nouns)):\n",
    "  result_nouns_stopwords.append([])\n",
    "\n",
    "for i in range(len(result_nouns)):\n",
    "  for word in result_nouns[i]:\n",
    "    if word not in stop_words:\n",
    "      result_nouns_stopwords[i].append(word) #result_nouns_stopwords\n",
    "df['result_nouns_stopwords'] = result_nouns_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sBPbUZkecHwH"
   },
   "outputs": [],
   "source": [
    "# 소문자로 바꾸기\n",
    "df.result_nouns_stopwords = df.result_nouns_stopwords.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YGRWtOKwcHwI"
   },
   "outputs": [],
   "source": [
    "# 특수문자 제거\n",
    "df.result_nouns_stopwords = df.result_nouns_stopwords.str.replace(pat=r'[^\\w]', repl=r' ', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fx7qw0YTcHwJ"
   },
   "outputs": [],
   "source": [
    "# 숫자 제거\n",
    "df.result_nouns_stopwords = df.result_nouns_stopwords.str.replace(r'\\d+','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xbMi1jZAcHwK"
   },
   "outputs": [],
   "source": [
    "# 길이가 짧은 단어 제거\n",
    "df.result_nouns_stopwords = df.result_nouns_stopwords.str.replace(r'\\W*\\b\\w{1,2}\\b','')\n",
    "result_nouns_stopwords = df.result_nouns_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I53QijHvcHwM"
   },
   "outputs": [],
   "source": [
    "# bigram 모델 생성\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "data_words_bigrams = make_bigrams(result_nouns_stopwords)\n",
    "id2word = corpora.Dictionary(data_words_bigrams)\n",
    "texts = data_words_bigrams\n",
    "corpus = [id2word.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yJ6ZuShxcHwN"
   },
   "outputs": [],
   "source": [
    "# LDA\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "ldamallet = LdaMallet(mallet_path, corpus=corpus, num_topics=30, id2word=id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZDOCF_xcHwO"
   },
   "outputs": [],
   "source": [
    "# coherence 평가\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "  \n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values\n",
    "\n",
    "#k = 10,20,30,40 평가 하였고 k = 30 선택\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_words_bigrams, start=10, limit=50, step=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzSsUnONcHwP"
   },
   "outputs": [],
   "source": [
    "# document 별 topic 확률 확인하는 dataframe 만들기\n",
    "idx,topic0,topic1,topic2,topic3,topic4,topic5,topic6,topic7,topic8,topic9=[],[],[],[],[],[],[],[],[],[],[]\n",
    "topic10,topic11,topic12,topic13,topic14,topic15,topic16,topic17,topic18,topic19, = [],[],[],[],[],[],[],[],[],[]\n",
    "topic20,topic21,topic22,topic23,topic24,topic25,topic26,topic27,topic28,topic29 = [],[],[],[],[],[],[],[],[],[]\n",
    "for i in range(len(corpus)):\n",
    "  document_topics = gensimmodel30.get_document_topics(corpus[i])\n",
    "  idx.append(i)\n",
    "  try:\n",
    "    topic0.append(document_topics[0][1])\n",
    "  except:\n",
    "    topic0.append(0)\n",
    "  try:\n",
    "    topic1.append(document_topics[1][1])\n",
    "  except:\n",
    "    topic1.append(0)\n",
    "  try:\n",
    "    topic2.append(document_topics[2][1])\n",
    "  except:\n",
    "    topic2.append(0)\n",
    "  try:\n",
    "    topic3.append(document_topics[3][1])\n",
    "  except:\n",
    "    topic3.append(0)\n",
    "  try:\n",
    "    topic4.append(document_topics[4][1])\n",
    "  except:\n",
    "    topic4.append(0)  \n",
    "  try:\n",
    "    topic5.append(document_topics[5][1])\n",
    "  except:\n",
    "    topic5.append(0)  \n",
    "  try:\n",
    "    topic6.append(document_topics[6][1])\n",
    "  except:\n",
    "    topic6.append(0)\n",
    "  try:\n",
    "    topic7.append(document_topics[7][1])\n",
    "  except:\n",
    "    topic7.append(0)\n",
    "  try:\n",
    "    topic8.append(document_topics[8][1])\n",
    "  except:\n",
    "    topic8.append(0)\n",
    "  try:\n",
    "    topic9.append(document_topics[9][1])\n",
    "  except:\n",
    "    topic9.append(0)\n",
    "  try:\n",
    "    topic10.append(document_topics[10][1])\n",
    "  except:\n",
    "    topic10.append(0)\n",
    "  try:\n",
    "    topic11.append(document_topics[11][1])\n",
    "  except:\n",
    "    topic11.append(0)\n",
    "  try:\n",
    "    topic12.append(document_topics[12][1])\n",
    "  except:\n",
    "    topic12.append(0)\n",
    "  try:\n",
    "    topic13.append(document_topics[13][1])\n",
    "  except:\n",
    "    topic13.append(0)\n",
    "  try:\n",
    "    topic14.append(document_topics[14][1])\n",
    "  except:\n",
    "    topic14.append(0)\n",
    "  try:\n",
    "    topic15.append(document_topics[15][1])\n",
    "  except:\n",
    "    topic15.append(0)\n",
    "  try:\n",
    "    topic16.append(document_topics[16][1])\n",
    "  except:\n",
    "    topic16.append(0)\n",
    "  try:\n",
    "    topic17.append(document_topics[17][1])\n",
    "  except:\n",
    "    topic17.append(0)\n",
    "  try:\n",
    "    topic18.append(document_topics[18][1])\n",
    "  except:\n",
    "    topic18.append(0)\n",
    "  try:\n",
    "    topic19.append(document_topics[19][1])\n",
    "  except:\n",
    "    topic19.append(0)\n",
    "  try:\n",
    "    topic20.append(document_topics[20][1])\n",
    "  except:\n",
    "    topic20.append(0)\n",
    "  try:\n",
    "    topic21.append(document_topics[21][1])\n",
    "  except:\n",
    "    topic21.append(0)\n",
    "  try:\n",
    "    topic22.append(document_topics[22][1])\n",
    "  except:\n",
    "    topic22.append(0)\n",
    "  try:\n",
    "    topic23.append(document_topics[23][1])\n",
    "  except:\n",
    "    topic23.append(0)\n",
    "  try:\n",
    "    topic24.append(document_topics[24][1])\n",
    "  except:\n",
    "    topic24.append(0)\n",
    "  try:\n",
    "    topic25.append(document_topics[25][1])\n",
    "  except:\n",
    "    topic25.append(0)\n",
    "\n",
    "  try:\n",
    "    topic26.append(document_topics[26][1])\n",
    "  except:\n",
    "    topic26.append(0)\n",
    "\n",
    "  try:\n",
    "    topic27.append(document_topics[27][1])\n",
    "  except:\n",
    "    topic27.append(0)\n",
    "\n",
    "  try:\n",
    "    topic28.append(document_topics[28][1])\n",
    "  except:\n",
    "    topic28.append(0)\n",
    "  try:\n",
    "    topic29.append(document_topics[29][1])\n",
    "  except:\n",
    "    topic29.append(0)\n",
    "df_topic_probs_df['idx'] = idx\n",
    "df_topic_probs_df['topic0'] = topic0\n",
    "df_topic_probs_df['topic1'] = topic1\n",
    "df_topic_probs_df['topic2'] = topic2\n",
    "df_topic_probs_df['topic3'] = topic3\n",
    "df_topic_probs_df['topic4'] = topic4\n",
    "df_topic_probs_df['topic5'] = topic5\n",
    "df_topic_probs_df['topic6'] = topic6\n",
    "df_topic_probs_df['topic7'] = topic7\n",
    "df_topic_probs_df['topic8'] = topic8\n",
    "df_topic_probs_df['topic9'] = topic9\n",
    "df_topic_probs_df['topic10'] = topic10\n",
    "df_topic_probs_df['topic11'] = topic11\n",
    "df_topic_probs_df['topic12'] = topic12\n",
    "df_topic_probs_df['topic13'] = topic13\n",
    "df_topic_probs_df['topic14'] = topic14\n",
    "df_topic_probs_df['topic15'] = topic15\n",
    "df_topic_probs_df['topic16'] = topic16\n",
    "df_topic_probs_df['topic17'] = topic17\n",
    "df_topic_probs_df['topic18'] = topic18\n",
    "df_topic_probs_df['topic19'] = topic19\n",
    "df_topic_probs_df['topic20'] = topic20\n",
    "df_topic_probs_df['topic21'] = topic21\n",
    "df_topic_probs_df['topic22'] = topic22\n",
    "df_topic_probs_df['topic23'] = topic23\n",
    "df_topic_probs_df['topic24'] = topic24\n",
    "df_topic_probs_df['topic25'] = topic25\n",
    "df_topic_probs_df['topic26'] = topic26\n",
    "df_topic_probs_df['topic27'] = topic27\n",
    "df_topic_probs_df['topic28'] = topic28\n",
    "df_topic_probs_df['topic29'] = topic29\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zLClnpsVcHwQ"
   },
   "outputs": [],
   "source": [
    "#document 별 topic 확률을 토대로 속성 확률 구하기\n",
    "df_topic_probs_df['att1'] = (df_topic_probs_df['topic1'] + df_topic_probs_df['topic2'] + df_topic_probs_df['topic3'] + df_topic_probs_df['topic4'] + df_topic_probs_df['topic5'] + df_topic_probs_df['topic6'] + df_topic_probs_df['topic7'] + df_topic_probs_df['topic8'] + df_topic_probs_df['topic9'] + df_topic_probs_df['topic11'] + df_topic_probs_df['topic12'] + df_topic_probs_df['topic13'] + df_topic_probs_df['topic14'] + + df_topic_probs_df['topic15']+ df_topic_probs_df['topic17']+ df_topic_probs_df['topic18']+ df_topic_probs_df['topic19']+ df_topic_probs_df['topic20']+ df_topic_probs_df['topic21']+ df_topic_probs_df['topic22']+ df_topic_probs_df['topic24']+ df_topic_probs_df['topic25']+ df_topic_probs_df['topic26']+ df_topic_probs_df['topic29']+df_topic_probs_df['topic28'])/25\n",
    "df_topic_probs_df['att2'] = (df_topic_probs_df['topic10']+df_topic_probs_df['topic16']+df_topic_probs_df['topic23'])/3\n",
    "df_topic_probs_df['att3'] = (df_topic_probs_df['topic0']+df_topic_probs_df['topic27'])/2\n",
    "\n",
    "df_topic_probs_df = df_topic_probs_df[['att1','att2','att3']]\n",
    "df_topic_probs_df_argmax = []\n",
    "for i in range(len(df_topic_probs_df)):\n",
    "  df_topic_probs_df_argmax.append(np.argmax(df_topic_probs_df.loc[i]))\n",
    "df_topic_probs_df['argmax'] = df_topic_probs_df['argmax'] +1\n",
    "df['attribute_num2'] = df_topic_probs_df.argmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4ZdIw5QcSPN"
   },
   "source": [
    "# 한글 댓글 SNA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ppNCiiu2cHwR"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import networkx as nx\n",
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GqbFNXYZcHwS"
   },
   "outputs": [],
   "source": [
    "num_top_nouns = 70\n",
    "comment_nouns_counter = Counter(comment_mecab_nouns_len_stp_fin)\n",
    "comment_top_nouns = dict(comment_nouns_counter.most_common(num_top_nouns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i7FXvgrwcHwS"
   },
   "outputs": [],
   "source": [
    "comment_word2id = {w: i for i, w in enumerate(comment_top_nouns.keys())}\n",
    "comment_id2word = {i: w for i, w in enumerate(comment_top_nouns.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBEYlT6vcHwT"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "comment_adjacent_matrix = np.zeros((num_top_nouns, num_top_nouns), int)\n",
    "for sentence in comment_mecab_nouns_len_stp_fin:\n",
    "    for wi, i in comment_word2id.items():\n",
    "        if wi in sentence:\n",
    "            for wj, j in comment_word2id.items():\n",
    "                if i != j and wj in sentence:\n",
    "                    comment_adjacent_matrix[i][j] += 1\n",
    "comment_adjacent_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bQTBD5bcHwU"
   },
   "outputs": [],
   "source": [
    "comment_network = nx.from_numpy_matrix(comment_adjacent_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6Eg7U8YcHwV"
   },
   "outputs": [],
   "source": [
    "# 폰트 확인\n",
    "!sudo apt-get install -y fonts-nanum\n",
    "!sudo fc-cache -fv\n",
    "!rm ~/.cache/matplotlib -rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tiF3WTEZcHwV"
   },
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "import matplotlib.pyplot as plt\n",
    "path='/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font_name = fm.FontProperties(fname=path,size=10).get_name()\n",
    "print(font_name)\n",
    "plt.rc('font',family=font_name)\n",
    "fm._rebuild()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dA6s5gQ9cHwV"
   },
   "outputs": [],
   "source": [
    "option = {\n",
    "    'node_color' : 'lightblue',\n",
    "    'node_size' : 2000\n",
    "}\n",
    "plt.figure(figsize=(20,20))\n",
    "nx.draw_spring(comment_network, labels=comment_id2word, font_family='NanumBarunGothic', **option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8pqmgtMvcHwW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "khM8mA6ycHwW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CckXaDkYcHwW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-7juL-oXcHwX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GQ7n7Q6gcHwX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tw4GYH4McHwX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DAqs0S1VcHwX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lkgnxLZcHwY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqBVhXyBcHwY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4-bm-sYcHwY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o8heLMkPcHwY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RxRvdd-ocHwZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DQJKaCOLcHwZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z-RFbuN1cHwZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUk9-ClbcHwZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gVoHHcylcHwZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oK5HviabcHwa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3iOJ5AKUcHwa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QaGJBZ6WcHwa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0PKaKdT4cHwa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CotWMpzWcHwa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_w-IReCxcHwb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RMsCCVyNcHwb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7aALQ_ALcHwb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bLw-WxbqcHwb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sxpTCU_UcHwb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "댓글 전처리와 LDA 모델링, SNA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

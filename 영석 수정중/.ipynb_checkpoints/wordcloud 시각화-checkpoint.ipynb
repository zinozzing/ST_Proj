{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM3YJNuh20pI"
   },
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import numpy as np\n",
    "from PIL import Image "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ww3jGrDN20pN"
   },
   "source": [
    "# 한국 웹툰 속성별 긍,부정 wordcloud "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZvvgIpb920pT"
   },
   "outputs": [],
   "source": [
    "ko_comment = pd.read_csv('/content/gdrive/MyDrive/webtoon sentiment analysis/ko_comment_mecab_nouns.csv')\n",
    "ko_df = pd.read_csv('/content/gdrive/Shareddrives/데청캠/웹툰 세계화/데이터 모음집/kw_temp0.csv')\n",
    "attr = pd.read_csv('/content/gdrive/Shareddrives/데청캠/웹툰 세계화/감성분석/ko_attribute.csv')\n",
    "sign = pd.read_csv('/content/gdrive/Shareddrives/데청캠/웹툰 세계화/데이터 모음집/to_hur.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uNaakrU320pY"
   },
   "outputs": [],
   "source": [
    "# 상위 20% 웹툰만 선정\n",
    "ko_df.y.sort_values(ascending=False)[:56].index\n",
    "ko_df = ko_df.loc[[149,  49, 192,  10,  48, 291,   0, 292,   1, 243, 242, 148,  52,\n",
    "            101, 250, 107,   2, 193,   3,  55, 102, 297, 196,  53, 295, 104,\n",
    "            252, 153, 198, 108,   4, 300, 154, 105, 244, 302, 195, 112,  60,\n",
    "            150, 152,  62, 204,  54, 247, 117, 293, 296, 245, 164, 248,  12,\n",
    "             50, 202, 155, 111]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtuvYFhV20pb"
   },
   "outputs": [],
   "source": [
    "ko_comment = pd.merge(ko_df[['code','title']],ko_comment,left_on = 'code',right_on='code_b',how='right')\n",
    "ko_comment = ko_comment[['code', 'title','ep_num_b', 'best_comment','comment_mecab_nouns', 'comment_mecab_nouns_len_stp']]\n",
    "ko_comment = pd.merge(ko_comment,attr[['code_b','ep_num_b','best_comment','attribute_num']],how='left', left_on=['code','ep_num_b','best_comment'],right_on = ['code_b','ep_num_b','best_comment'])\n",
    "ko_comment = ko_comment.drop(['comment_mecab_nouns','code_b'],axis=1)\n",
    "ko_comment = ko_comment.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FW-zZFtp20pd"
   },
   "outputs": [],
   "source": [
    "# 속성별로 사전 3개 합하여 최종 긍,부정 판별\n",
    "sign['signs'] = 0\n",
    "sign.loc[sign['movie_review']+sign['KNU_score_sum']+sign['fodo']>0,'signs'] = 'pos'\n",
    "sign.loc[sign['movie_review']+sign['KNU_score_sum']+sign['fodo']<0,'signs'] = 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywP43oU520pf"
   },
   "outputs": [],
   "source": [
    "ko = pd.merge(ko_comment,sign[['code_b','ep_num_b','best_comment','signs']],left_on=['code','ep_num_b','best_comment'],right_on = ['code_b','ep_num_b','best_comment'])\n",
    "ko = ko.drop(['code_b'],axis=1)\n",
    "max_pg = ko.pivot_table(index='title',aggfunc='max',values='ep_num_b').reset_index()\n",
    "ko = pd.merge(ko,max_pg,on='title',how='left')\n",
    "#최근의 10화만 사용\n",
    "ko = ko[ko.ep_num_b_y-ko.ep_num_b_x<10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1V3Nmq920pj"
   },
   "outputs": [],
   "source": [
    "# 스토리 속성 긍정\n",
    "pos1 = ko[(ko.attribute_num==1) & (ko.signs=='pos')] \n",
    "comment1 = pos1.comment_mecab_nouns_len_stp\n",
    "comment1 = comment1.reset_index()\n",
    "comment1 = comment1.iloc[:,1:]\n",
    "result=[]\n",
    "for i in range(len(comment1)):\n",
    "  result.append(comment1.comment_mecab_nouns_len_stp[i])\n",
    "result = str(result)\n",
    "result = result.replace(\"'\",'')\n",
    "result = result.replace('\"','')\n",
    "result = result.replace('[','')\n",
    "result = result.replace(']','')\n",
    "result1 = result.split(', ')\n",
    "\n",
    "#word cloud 생성\n",
    "#wc = WordCloud(background_color = 'black', max_words = 50,font_path=path,colormap='OrRd')\n",
    "wc = WordCloud(background_color = 'white', max_words = 50,font_path=path) \n",
    "wc = wc.generate_from_frequencies(Counter(result1)) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off') #on/off\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vxfIaqti20pl"
   },
   "outputs": [],
   "source": [
    "# 스토리 속성 부정\n",
    "neg1 = ko[(ko.attribute_num==1) & (ko.signs=='neg')] \n",
    "comment1 = neg1.comment_mecab_nouns_len_stp\n",
    "comment1 = comment1.reset_index()\n",
    "comment1 = comment1.iloc[:,1:]\n",
    "result=[]\n",
    "for i in range(len(comment1)):\n",
    "  result.append(comment1.comment_mecab_nouns_len_stp[i])\n",
    "result = str(result)\n",
    "result = result.replace(\"'\",'')\n",
    "result = result.replace('\"','')\n",
    "result = result.replace('[','')\n",
    "result = result.replace(']','')\n",
    "result1 = result.split(', ')\n",
    "\n",
    "#word cloud 생성\n",
    "wc = WordCloud(background_color = 'black', max_words = 50,font_path=path,colormap='OrRd')\n",
    "#wc = WordCloud(background_color = 'white', max_words = 50,font_path=path) \n",
    "wc = wc.generate_from_frequencies(Counter(result1)) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off') #on/off\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MJeC6E0e20pp"
   },
   "outputs": [],
   "source": [
    "# 캐릭터 속성 긍정\n",
    "pos2 = ko[(ko.attribute_num==2) & (ko.signs=='pos')] \n",
    "comment1 = pos2.comment_mecab_nouns_len_stp\n",
    "comment1 = comment1.reset_index()\n",
    "comment1 = comment1.iloc[:,1:]\n",
    "result=[]\n",
    "for i in range(len(comment1)):\n",
    "  result.append(comment1.comment_mecab_nouns_len_stp[i])\n",
    "result = str(result)\n",
    "result = result.replace(\"'\",'')\n",
    "result = result.replace('\"','')\n",
    "result = result.replace('[','')\n",
    "result = result.replace(']','')\n",
    "result1 = result.split(', ')\n",
    "\n",
    "#word cloud 생성\n",
    "#wc = WordCloud(background_color = 'black', max_words = 50,font_path=path,colormap='OrRd')\n",
    "wc = WordCloud(background_color = 'white', max_words = 50,font_path=path) \n",
    "wc = wc.generate_from_frequencies(Counter(result1)) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off') #on/off\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RfhU091Y20pq"
   },
   "outputs": [],
   "source": [
    "#캐릭터 속성 부정\n",
    "neg2 = ko[(ko.attribute_num==2) & (ko.signs=='neg')] \n",
    "comment1 = neg1.comment_mecab_nouns_len_stp\n",
    "comment1 = comment1.reset_index()\n",
    "comment1 = comment1.iloc[:,1:]\n",
    "result=[]\n",
    "for i in range(len(comment1)):\n",
    "  result.append(comment1.comment_mecab_nouns_len_stp[i])\n",
    "result = str(result)\n",
    "result = result.replace(\"'\",'')\n",
    "result = result.replace('\"','')\n",
    "result = result.replace('[','')\n",
    "result = result.replace(']','')\n",
    "result1 = result.split(', ')\n",
    "\n",
    "#word cloud 생성\n",
    "#wc = WordCloud(background_color = 'black', max_words = 50,font_path=path,colormap='OrRd')\n",
    "wc = WordCloud(background_color = 'white', max_words = 50,font_path=path) \n",
    "wc = wc.generate_from_frequencies(Counter(result1)) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off') #on/off\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AbC9GV-_20pt"
   },
   "outputs": [],
   "source": [
    "# 작가 속성 긍정\n",
    "pos3 = ko[(ko.attribute_num==3) & (ko.signs=='pos')] \n",
    "comment1 = pos3.comment_mecab_nouns_len_stp\n",
    "comment1 = comment1.reset_index()\n",
    "comment1 = comment1.iloc[:,1:]\n",
    "result=[]\n",
    "for i in range(len(comment1)):\n",
    "  result.append(comment1.comment_mecab_nouns_len_stp[i])\n",
    "result = str(result)\n",
    "result = result.replace(\"'\",'')\n",
    "result = result.replace('\"','')\n",
    "result = result.replace('[','')\n",
    "result = result.replace(']','')\n",
    "result1 = result.split(', ')\n",
    "\n",
    "#word cloud 생성\n",
    "#wc = WordCloud(background_color = 'black', max_words = 50,font_path=path,colormap='OrRd')\n",
    "wc = WordCloud(background_color = 'white', max_words = 50,font_path=path) \n",
    "wc = wc.generate_from_frequencies(Counter(result1)) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off') #on/off\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DOWxSvEE20pw"
   },
   "outputs": [],
   "source": [
    "# 작가 속성 부정\n",
    "neg3 = ko[(ko.attribute_num==3) & (ko.signs=='neg')] \n",
    "comment1 = neg3.comment_mecab_nouns_len_stp\n",
    "comment1 = comment1.reset_index()\n",
    "comment1 = comment1.iloc[:,1:]\n",
    "result=[]\n",
    "for i in range(len(comment1)):\n",
    "  result.append(comment1.comment_mecab_nouns_len_stp[i])\n",
    "result = str(result)\n",
    "result = result.replace(\"'\",'')\n",
    "result = result.replace('\"','')\n",
    "result = result.replace('[','')\n",
    "result = result.replace(']','')\n",
    "result1 = result.split(', ')\n",
    "\n",
    "#word cloud 생성\n",
    "wc = WordCloud(background_color = 'black', max_words = 50,font_path=path,colormap='OrRd')\n",
    "wc = WordCloud(background_color = 'white', max_words = 50,font_path=path) \n",
    "wc = wc.generate_from_frequencies(Counter(result1)) \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off') #on/off\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9an55XH20py"
   },
   "source": [
    "# 북미 댓글 속성별 wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2-0FtfH20pz"
   },
   "outputs": [],
   "source": [
    "en_comments = pd.read_csv('/content/gdrive/MyDrive/webtoon sentiment analysis/en_comment_forLDA.csv')\n",
    "en = pd.read_csv('/content/gdrive/Shareddrives/데청캠/웹툰 세계화/데이터 모음집/ew_2_comment_allDic.csv')\n",
    "ew_total = pd.read_csv('/content/gdrive/Shareddrives/데청캠/웹툰 세계화/데이터 모음집/ew_temp0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IMDqhy1u20p0"
   },
   "outputs": [],
   "source": [
    "en = en.iloc[:,1:]\n",
    "en = en.reset_index()\n",
    "en = en.iloc[:,1:]\n",
    "for i in range(len(en)):\n",
    "  if en['best_comment'][i][:3]=='TOP':\n",
    "    en['best_comment'][i] = en['best_comment'][i][3:]\n",
    "\n",
    "# 댓글 긍부정 파악\n",
    "en['signs'] = 0\n",
    "en.loc[en['imdb']+en['FoDoDic']+en['senti_pos-neg']>0,'signs'] = 'pos'\n",
    "en.loc[en['imdb']+en['FoDoDic']+en['senti_pos-neg']<0,'signs'] = 'neg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etVvzXop20p1"
   },
   "outputs": [],
   "source": [
    "en_comments = en_comments[['title_a','ep_title_a','num_list','result_nouns_stopwords_lemm_str']]\n",
    "en_comments.num_list = en_comments.num_list.str[1:]\n",
    "en_comments.num_list = en_comments.num_list.astype(int)\n",
    "en_comments = en_comments[['title_a','num_list','best_comment','result_nouns_stopwords_lemm_str']]\n",
    "en = pd.merge(en[['title_a', 'num_list', 'best_comment', 'attribute',\t'signs']],en_comments[['title_a', 'num_list', 'best_comment', 'result_nouns_stopwords_lemm_str']],\n",
    "         left_on = ['title_a', 'num_list', 'best_comment'],right_on = ['title_a', 'num_list', 'best_comment'],how='left')\n",
    "en = en[['title_a','num_list','best_comment','result_nouns_stopwords_lemm_str','attribute','signs']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tU76Oa3B20p2"
   },
   "outputs": [],
   "source": [
    "# 북미 웹툰 상위 20%만 선정 \n",
    "ew_total.y.sort_values(ascending=False)[:37].index\n",
    "ew_total = ew_total.loc[[125,  35, 194,  72, 195,  36, 132,   2, 135,  38,  29,  73, 166,\n",
    "              0, 192, 190, 197, 105, 198, 133, 134, 104,  33,   6,  37,  74,\n",
    "             43, 162,   1, 107, 199, 196, 201, 167, 136, 168,  30]]\n",
    "ew_maxep = ew_total[['title','max_ep']]\n",
    "ew_title = ew_total['title'].tolist()\n",
    "en_comments['top'] = 0\n",
    "for i in range(len(en_comments)):\n",
    "  if en_comments.title_a[i] in ew_title:\n",
    "    en_comments['top'][i] = 1\n",
    "en_comments = en_comments[en_comments.top==1]\n",
    "en_comments = en_comments[['title_a','num_list','best_comment','result_nouns_stopwords_lemm_str']]\n",
    "en = pd.merge(en_comments,en,left_on = ['title_a',\t'num_list',\t'best_comment'],right_on =['title_a','num_list','best_comment'], )\n",
    "en = en[['title_a', 'num_list', 'best_comment','result_nouns_stopwords_lemm_str_x','attribute','signs']]\n",
    "ew_maxep = ew_maxep.reset_index()\n",
    "ew_maxep = ew_maxep.iloc[:,1:]\n",
    "en1 = pd.merge(en,ew_maxep,left_on='title_a',right_on='title',how='left')\n",
    "en1 = en1.dropna()\n",
    "en1 = en1.reset_index()\n",
    "en1 = en1.iloc[:,1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LU_jOqP620p3"
   },
   "outputs": [],
   "source": [
    "# 최근 10화만 선정\n",
    "en1 = en1[en1.max_ep - en1.num_list < 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqlGRnrg20p6"
   },
   "outputs": [],
   "source": [
    "# 스토리 속성 word cloud\n",
    "en_att1 = en1[en1.attribute==1]\n",
    "en_att1 = en_att1.reset_index()\n",
    "en_att1 = en_att1.iloc[:,1:]\n",
    "comment1 = en_att1.result_nouns_stopwords_lemm_str\n",
    "comment1 = comment1.reset_index()\n",
    "comment1 = comment1.iloc[:,1:]\n",
    "result=[]\n",
    "for i in range(len(comment1)):\n",
    "  result.append(comment1.result_nouns_stopwords_lemm_str[i])\n",
    "result = str(result)\n",
    "result = result.replace(\"'\",'')\n",
    "result = result.replace('\"','')\n",
    "result = result.replace('[','')\n",
    "result = result.replace(']','')\n",
    "result = result.replace(',',' ')\n",
    "result = result.replace('  ',' ')\n",
    "result1 = result.split(' ')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "result2 = []\n",
    "for w in result1: \n",
    "    if w not in stop_words: \n",
    "        result2.append(w) \n",
    "\n",
    "wc = WordCloud(background_color = 'white', max_words = 50,font_path=path)\n",
    "wc = wc.generate_from_frequencies(Counter(result2)) #개수 기반으로 \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off') #on/off\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OSsiwqt620p7"
   },
   "outputs": [],
   "source": [
    "# 캐릭터 속성 wordcloud\n",
    "en_att2 = en1[en1.attribute==2]\n",
    "en_att2 = en_att2.reset_index()\n",
    "en_att2 = en_att2.iloc[:,1:]\n",
    "comment1 = en_att2.result_nouns_stopwords_lemm_str\n",
    "comment1 = comment1.reset_index()\n",
    "comment1 = comment1.iloc[:,1:]\n",
    "result=[]\n",
    "for i in range(len(comment1)):\n",
    "  result.append(comment1.result_nouns_stopwords_lemm_str[i])\n",
    "result = str(result)\n",
    "result = result.replace(\"'\",'')\n",
    "result = result.replace('\"','')\n",
    "result = result.replace('[','')\n",
    "result = result.replace(']','')\n",
    "result = result.replace(',',' ')\n",
    "result = result.replace('  ',' ')\n",
    "result1 = result.split(' ')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "result2 = []\n",
    "for w in result1: \n",
    "    if w not in stop_words: \n",
    "        result2.append(w) \n",
    "\n",
    "wc = WordCloud(background_color = 'white', max_words = 50,font_path=path)\n",
    "wc = wc.generate_from_frequencies(Counter(result2)) #개수 기반으로 \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off') #on/off\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h9-az6iI20p8"
   },
   "outputs": [],
   "source": [
    "# 작가 속성 wordcloud\n",
    "en_att3 = en1[en1.attribute==3]\n",
    "en_att3 = en_att3.reset_index()\n",
    "en_att3 = en_att3.iloc[:,1:]\n",
    "comment1 = en_att3.result_nouns_stopwords_lemm_str\n",
    "comment1 = comment1.reset_index()\n",
    "comment1 = comment1.iloc[:,1:]\n",
    "result=[]\n",
    "for i in range(len(comment1)):\n",
    "  result.append(comment1.result_nouns_stopwords_lemm_str[i])\n",
    "result = str(result)\n",
    "result = result.replace(\"'\",'')\n",
    "result = result.replace('\"','')\n",
    "result = result.replace('[','')\n",
    "result = result.replace(']','')\n",
    "result = result.replace(',',' ')\n",
    "result = result.replace('  ',' ')\n",
    "result1 = result.split(' ')\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords  \n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english')) \n",
    "result2 = []\n",
    "for w in result1: \n",
    "    if w not in stop_words: \n",
    "        result2.append(w) \n",
    "\n",
    "wc = WordCloud(background_color = 'white', max_words = 50,font_path=path)\n",
    "wc = wc.generate_from_frequencies(Counter(result2)) #개수 기반으로 \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(12,12))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off') #on/off\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "wordcloud 시각화.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
